##### 학습 목표: Neural Networks의 최적화 방법에 대해서 학습한다.
#### 방법론 01. Activation Functions
##### Activation Function 
- 어떤 값이 Neural Networks에 들어갔을 때, 일정 기준을 넘지 못하면 다음 Neural으로 보내지 않음
- Activation fuction이 없는 Neural Network는 선형분류기와 다를게 없음
##### Activation Function의 종류 
##### 1. Sigmoid 
- 숫자를 [0,1]사이로 꾸겨넣음
- 뉴런의 행동과 비슷해서 많이 쓰여옴
  
※ 3가지 문제점
1. 값이 큰 경우 얻은 (upstream)gradient를 죽임(kill)
2. Sigmoid 출력의 평균이 0이 아님(0.5)
3. exp() 계산이 복잡함
   
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/082a4ab8-51c5-4734-baee-8f96d5e660b8)

- What happens when x = -10? → local gradient = 0
- What happens when x = 0? → local gradient = 1.xx
- What happens when x = 10? → local gradient = 0

- local gradient = 0이면, upstream gradient 값에 상관없이 downstream gradient가 0이 됨(upstream gradient를 kill) → 학습 X
- Sigmoid 통과한 값이 무조건 0보다 크기 때문에, backprop시 gradient의 부호는 항상 입력 값 x에 의존하게 되는 문제 → activation function의 평균 출력을 0으로 맞추자 → tanh 도입

##### 2. tanh
- [-1, 1]
- 평균 출력 0
- 하지만, 여전히 gradient를 없애버림
  
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/a895528f-9508-41f7-883d-9a7476b0ab25)

##### 3. ReLU
- f(x) = max(0, x)
- 계산 효율성, sigmoid/tanh에 비해 빠름
- 평균 출력이 0이 X
- x < 0, gradient = 0
- x > 0, gradient = 1
- What happens when x = -10, gradient = 0
- What happens when x = 0, gradient = 0
- What happens when x = 10, gradient = 1
- Data cloud 관점에서 x가 음수면, ReLU activation function이 활성화 되지 않아서 학습 x

![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/46421d9a-0224-412e-ad4c-fe8a93b73151)

##### 4. Leaky ReLU
- x가 음수여도 local gradient가 0이라 upstream gradient를 kill하는 경우를 없애기 위해 도입

![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/0bf9ffa3-6620-4023-9028-4ad07e38e3f5)

##### 실제로, 
- ReLU를 쓰기 다만, learning rates 설정에 주의
- 한계치를 쥐어짜고 싶다면, Leaky ReLU등 시도
- sigmoid 나 tanh 는 쓰지 말것
  
#### 방법론 02. Weight Initialization

##### First idea: Small random numbers
- W = 0.01 * np.random.randn(Din, Dout)
    - 정규분포를 따름 (평균:0, 표준편차:1)
    - 0.01, 0.002, -0.002, 2.0 ... (0)
    - 3.0, -4,0 ... (매우 드뭄)
    - small network에선 괜찮지만, deeper networks에서는 문제가 생김

##### Weight Initialization: Activation statics 

###### initial weights from -0.01~0.01
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/17046b32-c85d-4913-87da-39e712be44f8)

###### initial weights from 0.01~0.05
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/a52b1c21-df53-4d3e-bf03-d0dd6b09cd85)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/e3830a36-0584-4f9d-8813-6ce3e60698e5)

##### Weight Initialization: What about tanh? → "Xavier" Initialization
- std: 1/sqrt(Din)
- W = np.random.randn(Din, Dout)/np.sqrt(Din)
- For conv layers, Din = filter_size^2 * input_channels
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/6d709274-7456-4abd-b256-a154f4c52573)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/a8544c41-7894-4ebc-a1cf-35bb7e346a94)

##### Weight Initialization: What about ReLU? → "Kaiming He" Initialization
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/fd713282-4cf6-4b11-8f67-a00f10109be8)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/9af1015e-b6bf-4b87-aa3c-b8b135438499)
- std: sqrt(2/Din)
- W = np.random.randn(Din, Dout) * np.sqrt(2/Din)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/52ec4065-a875-448d-921b-7055ebc26b22)

#### 방법론 03. Optimizers
- Vanilla Gradient Descent
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/c284ecc6-0a8e-43f7-a785-0dc11c8b6b83)

##### SGD의 문제점
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/510d1c2b-070d-48ce-bd3e-f40b6be08f2c)
- local minima or saddle point에 빠질 수 있음
- local minima: 지역적으로 봤을 때, 가장 작아 보이는 값
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/e9b91ccd-ab80-4463-bf36-d734d5b09be6)

- saddle point: 기울기가 0인 지점 (고차원 데이터에 saddle point가 많음)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/29150bcc-9e9c-4c90-98c8-0be6a625b364)

##### SGD + Momentum
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/480a040a-ec9f-4b80-81a4-694b7cd0d5b5)

##### AdaGrad
※ Adaptive learning rates: 누적된 gradient(dx) 크기로 learning rate를 나눔
- x -= learning_rate * dx / (np.sqrt(dx*dx) + 1e-7)
  - dx*dx: 처음에는 속도가 빠르지만 학습할수 속도 줄이기 위함
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/4a0e22a1-ae84-4c9c-9e81-62e4b892f6c6)
- What happens to the step size over long time? Gradient의 누적 합이 매우 커져서, 학습이 되지 않음. → weight decay로 해결(RMSProp)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/446c47ea-a70a-46a0-96d6-e157a9c369c4)

##### Adam
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/604142d0-56de-467f-ad21-1a22afc2a94a)

1. 관성 이용 (local minima & saddle point에 빠지지 않게함)
2. Adaptive learning rate로 누적된 gradient의 크기에 최적화 (갈수록 lr ↓)
3. RMSProp의 weight decay로 학습이 멈지 않음 (2번의 문제점 해결)
![image](https://github.com/LeeGaHyeon/Study_deeplearning/assets/50908451/0e0b02e6-214b-44cf-9a08-14a10b8b4e44)


##### 출처: 한림대학교 2023년도 2학기 영상처리와 딥러닝 수업자료
